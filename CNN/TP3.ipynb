{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "# Import libraries and modules\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "np.random.seed(123) # for reproducibility\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Reshape, Dense, Activation,Flatten, Dropout, Convolution2D, MaxPooling2D, Input\n",
    "from utilitaire import affiche\n",
    "\n",
    "\n",
    "##################################################\n",
    "# I - Load pre-shuffled MNIST data train and test sets\n",
    "##################################################\n",
    "from tensorflow.keras.datasets.mnist import load_data\n",
    "from matplotlib import pyplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Il y a 60 000 images de 28x28 pixels dans la base de données. Lese labels sont des entiers entre 0 et 9.\n",
    " Les images sont des matrices de pixels de taille 28x28. Chaque pixel est un entier entre 0 et 255. : ils representent un chiffre de 0 a 9.\n",
    "\n",
    "\n",
    " - TRAIN = 60 000 images\n",
    " - TEST = 10 000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = load_data() # Chargement des données de MNIST\n",
    "\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "\n",
    "print(\"===========================================\")\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Permet de ne pas prendre toute la base !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on divise la base de train en 2 : 10% pour la validation et 90% pour l'entrainement\n",
    "X_train, pipo, y_train, pipo = train_test_split(X_train, y_train, test_size=0.9)\n",
    "\n",
    "#on divise la base de test en 2 : 10% pour la validation et 90% pour l'entrainement\n",
    "X_test, pipo, y_test, pipo = train_test_split(X_test, y_test, test_size=0.9)\n",
    "\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)\n",
    "\n",
    "print(\"X_test original shape\", X_test.shape)\n",
    "print(\"y_test original shape\", y_test.shape)\n",
    "\n",
    "#pipo\n",
    "print(\"pipo original shape\", pipo.shape)\n",
    "\n",
    "\n",
    "#shape d'une image\n",
    "print(X_train[1,:].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(2):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.imshow(X_train[i,:].reshape([28,28]), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AVANT : (60000, 28, 28)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "#APRES : (60000, 28, 28, 1)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "\n",
    "Y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "#ainsi, la classe 1 devient [0,1,0,0,0,0,0,0,0,0]\n",
    "#la classe 2 devient [0,0,1,0,0,0,0,0,0,0] ... alors avant c'était juste 1,2,3,4,5,6,7,8,9 ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "\n",
    "1) Le code load la base MNSIT avec ses 60 000 images. On divise la base en 2 parties : une partie d'apprentissage (train) et une partie de test (test). On affiche la taille de la base d'apprentissage et de test.\n",
    "\n",
    "2) Il y a 10 classes, de 0 a 9. \n",
    "\n",
    "3) Pré traitement :\n",
    "\n",
    "- On reshape les images\n",
    "- On les met en float\n",
    "- On les normalise\n",
    "- on les met en one hot encoding\n",
    "\n",
    "\n",
    "Les reshape permettent de mettre les images en 28x28x1. pour avoir la profondeur (niveau de gris) de l'image.\n",
    "Les images sont en float pour pouvoir les comparer entre autres reseaux de neurones. (avoir la mm précision + utilise moins de mémoire vive)\n",
    "\n",
    "\n",
    "Cela permet de faciliter l'apprentissage du reseau de neurones.\n",
    "\n",
    "\n",
    "4) la fonction tf.keras.utils.to_categorical permet de faire du one hot encoding. au lieu d'avoir un label de 0 a 9, on a un vecteur de 10 cases avec des 0 et un 1 a la case correspondant au label.\n",
    "\n",
    "exemple : 5 -> [0,0,0,0,0,1,0,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation d'un réseau de neuronnes\n",
    "\n",
    "- ici, on cree un reseau de neurones avec 2 couches cachées de 512 neurones chacune.\n",
    "\n",
    "\n",
    "- L'activation softmax permet de normaliser les valeurs de sortie entre 0 et 1. Cela permet de les interpreter comme des probabilités.\n",
    "- La commande Flatten permet de mettre les images en 1D. (28x28x1 -> 784x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "print(x.shape)\n",
    "x=Flatten()(x)\n",
    "print(x.shape)\n",
    "outputs=Dense(10, activation='softmax')(x)\n",
    "\n",
    "#cela signifie que l'on a 28*28 neurones en entrée et 10 neurones en sortie\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()\n",
    "\n",
    "#nombre de param\n",
    "print(\"Nombre de paramètres : \", model.count_params())\n",
    "\n",
    "# nombre de param par le calcul\n",
    "print(28*28*10 + 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.1\n",
    "batch_size= 32\n",
    "epochs=50\n",
    "\n",
    "#on definit le type de descente de gradient\n",
    "sgd1= tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "\n",
    "#cela permet de calculer l'erreur de classification\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
    "tps1 = time.time()\n",
    "\n",
    "\n",
    "history =model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_test, Y_test))\n",
    "tps2 = time.time()\n",
    "#print(history.history.keys())\n",
    "\n",
    "affiche(history)\n",
    "print('lr=',lr,'batch_size=',batch_size, 'epochs=',epochs)\n",
    "print('Temps d apprentissage',tps2 - tps1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lr : learning rate , c'est le pas d'apprentissage. C'est la taille du pas que l'on fait pour trouver le minimum de la fonction de cout.\n",
    "\n",
    "- batch_size : nombre d'images que l'on va donner au reseau de neurones avant de faire une mise a jour des poids.\n",
    "\n",
    "- epochs : nombre de fois que l'on va faire passer toutes les images dans le reseau de neurones.\n",
    "\n",
    "\n",
    "__Fonction de cout : categorical_crossentropy__\n",
    "\n",
    "**categorical_crossentropy: Used as a loss function for multi-class classification model where there are two or more output labels. The output label is assigned one-hot category encoding value in form of 0s and 1. The output label, if present in integer form, is converted into categorical encoding using keras.utils to_categorical method.**\n",
    "\n",
    "Ici, la categorical_crossentropy est utilisée car on a 10 classes. (0 a 9)\n",
    "\n",
    "\n",
    "\n",
    "La fonction affiche(history) permet d'afficher les courbes d'apprentissage. On peut voir l'evolution de la fonction de cout et de la precision en fonction du nombre d'epochs.\n",
    "\n",
    "\n",
    "# Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) model.predict permet de predire les labels des images de test. \n",
    "\n",
    "2) on utilise y_pred.argmax car on a fait du one hot encoding. On veut donc recuperer la position du 1 dans le vecteur.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN : Sans momentum\n",
    "\n",
    "\n",
    "On ajoute une couche cachée a 256 neuronnes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "x=Flatten()(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "outputs=Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_couche_cachee = Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "#nombre de param\n",
    "print(\"Nombre de paramètres : \", model_couche_cachee.count_params())\n",
    "print(256*(28*28+1) + 10*(256+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.01\n",
    "batch_size= 64\n",
    "epochs=50\n",
    "\n",
    "#on definit le type de descente de gradient : on ajoute le momentum\n",
    "\n",
    "sgd1= tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "\n",
    "\n",
    "#cela permet de calculer l'erreur de classification\n",
    "model_couche_cachee.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
    "tps1 = time.time()\n",
    "\n",
    "\n",
    "history =model_couche_cachee.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_test, Y_test))\n",
    "tps2 = time.time()\n",
    "#print(history.history.keys())\n",
    "\n",
    "affiche(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_couche_cachee.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajout momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.01\n",
    "batch_size= 64\n",
    "epochs=50\n",
    "\n",
    "#on definit le type de descente de gradient : on ajoute le momentum\n",
    "\n",
    "sgd1= tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "#cela permet de calculer l'erreur de classification\n",
    "model_couche_cachee.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
    "tps1 = time.time()\n",
    "\n",
    "\n",
    "history =model_couche_cachee.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_test, Y_test))\n",
    "tps2 = time.time()\n",
    "#print(history.history.keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model_couche_cachee.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avec Drop OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "x=Dropout(0.4)(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "\n",
    "outputs=Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_couche_cachee_dropout = Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "#nombre de param\n",
    "print(\"Nombre de paramètres : \", model_couche_cachee_dropout.count_params())\n",
    "print(256*(28*28+1) + 10*(256+1))\n",
    "\n",
    "\n",
    "lr= 0.1\n",
    "batch_size= 32\n",
    "epochs=50\n",
    "\n",
    "#on definit le type de descente de gradient : on ajoute le momentum\n",
    "\n",
    "sgd1= tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "#cela permet de calculer l'erreur de classification\n",
    "model_couche_cachee_dropout.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
    "tps1 = time.time()\n",
    "\n",
    "\n",
    "history =model_couche_cachee_dropout.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_test, Y_test))\n",
    "tps2 = time.time()\n",
    "#print(history.history.keys())\n",
    "\n",
    "affiche(history)\n",
    "print('lr=',lr,'batch_size=',batch_size, 'epochs=',epochs)\n",
    "print('Temps d apprentissage',tps2 - tps1)\n",
    "\n",
    "\n",
    "print(\"=========================================== EVALUATION ===========================================\")\n",
    "score = model_couche_cachee_dropout.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le drop out evite l'overfitting !!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajout d'une seconde couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "x=Flatten()(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "x=Dropout(0.5)(x)\n",
    "outputs=Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_2_couche_cachee_dropout = Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "#nombre de param\n",
    "print(\"Nombre de paramètres : \", model_2_couche_cachee_dropout.count_params())\n",
    "\n",
    "\n",
    "lr= 0.1\n",
    "batch_size= 128\n",
    "epochs=50\n",
    "\n",
    "#on definit le type de descente de gradient : on ajoute le momentum\n",
    "\n",
    "sgd1= tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "#cela permet de calculer l'erreur de classification\n",
    "model_2_couche_cachee_dropout.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
    "tps1 = time.time()\n",
    "\n",
    "\n",
    "history =model_2_couche_cachee_dropout.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_test, Y_test))\n",
    "tps2 = time.time()\n",
    "#print(history.history.keys())\n",
    "\n",
    "affiche(history)\n",
    "print('lr=',lr,'batch_size=',batch_size, 'epochs=',epochs)\n",
    "print('Temps d apprentissage',tps2 - tps1)\n",
    "\n",
    "#evaluation\n",
    "\n",
    "\n",
    "score = model_2_couche_cachee_dropout.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "#ajout convolution\n",
    "x=Convolution2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "x=Convolution2D(64, kernel_size=(3,3), activation='relu')(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "outputs=Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_CNN = Model(inputs, outputs)\n",
    "\n",
    "print(\"Nombre de paramètres : \", model_CNN.count_params())\n",
    "\n",
    "model_CNN.summary()\n",
    "\n",
    "print(\"couche 1 a \", 32*(3*3*1+1), \" paramètres\")\n",
    "print(\"couche 2 a \", 64*(3*3*32+1), \" paramètres\")\n",
    "\n",
    "print(\"MLP a \", 256*(9216+1), \" paramètres\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr= 0.01\n",
    "batch_size= 64\n",
    "epochs=50\n",
    "\n",
    "#on definit le type de descente de gradient : on ajoute le momentum\n",
    "\n",
    "sgd1= tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "#cela permet de calculer l'erreur de classification\n",
    "model_CNN.compile(loss='categorical_crossentropy', optimizer=sgd1, metrics=['accuracy'])\n",
    "tps1 = time.time()\n",
    "\n",
    "\n",
    "history =model_CNN.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, verbose=1,validation_data=(X_test, Y_test))\n",
    "\n",
    "affiche(history)\n",
    "\n",
    "#evaluation\n",
    "\n",
    "\n",
    "score = model_CNN.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.argmax(axis=-1)\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Ajout Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28,28,1))\n",
    "x = inputs\n",
    "#ajout convolution\n",
    "x=Convolution2D(32, kernel_size=(3,3), activation='relu')(x)\n",
    "x=Convolution2D(64, kernel_size=(3,3), activation='relu')(x)\n",
    "#max pooling\n",
    "x=MaxPooling2D(pool_size=(2,2))(x)\n",
    "x=Flatten()(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "x=Dense(256, activation='relu')(x)\n",
    "outputs=Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_CNN_max = Model(inputs, outputs)\n",
    "\n",
    "print(\"Nombre de paramètres : \", model_CNN_max.count_params())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
